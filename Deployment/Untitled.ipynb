{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcace4f6-a110-40fc-b323-bfcea6205e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'sklearn.pipeline.Pipeline'>\n",
      "Has predict_proba? True\n",
      "Has classes_? True\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "model_path = \"models/logreg_pipeline.pkl\"  # adjust path if needed\n",
    "pipeline = joblib.load(model_path)\n",
    "\n",
    "print(\"Type:\", type(pipeline))\n",
    "print(\"Has predict_proba?\", hasattr(pipeline, \"predict_proba\"))\n",
    "print(\"Has classes_?\", hasattr(pipeline, \"classes_\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dec624a-dbdf-48b7-98a2-0334f64eed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf': LogisticRegression(max_iter=1000, random_state=42),\n",
      " 'tfidf': TfidfVectorizer(max_features=5000, stop_words='english')}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Load your model\n",
    "import joblib\n",
    "pipeline = joblib.load(\"models/logreg_pipeline.pkl\")\n",
    "\n",
    "# Show pipeline steps\n",
    "pprint(pipeline.named_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be85016e-0536-4161-8b38-2ef7b23400c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " non-suicide       0.93      0.94      0.93     23208\n",
      "     suicide       0.94      0.93      0.93     23207\n",
      "\n",
      "    accuracy                           0.93     46415\n",
      "   macro avg       0.93      0.93      0.93     46415\n",
      "weighted avg       0.93      0.93      0.93     46415\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[21866  1342]\n",
      " [ 1722 21485]]\n",
      "\n",
      "ROC AUC: 0.9809897585252059\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# ── 1) Reload & split data ────────────────────────────────────────────────\n",
    "df = pd.read_csv(\"data/raw.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "X = df[\"text\"]\n",
    "y = df[\"class\"]\n",
    "\n",
    "le        = LabelEncoder().fit(y)\n",
    "y_encoded = le.transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# ── 2) Load your saved pipeline ─────────────────────────────────────────\n",
    "pipeline = joblib.load(\"models/logreg_pipeline.pkl\")\n",
    "\n",
    "# ── 3) Predict & score ───────────────────────────────────────────────────\n",
    "y_pred  = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]   # “1” is your suicide class\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nROC AUC:\", roc_auc_score(y_test, y_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4eab31d-5cf3-417e-b144-d5793ab78094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.48.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (4.66.5)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (24.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (4.14.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from numba>=0.54->shap) (0.43.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e360e999-2b18-4d7c-87db-4466fecde6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ proto_pipe defined with steps: ['enrich', 'union', 'clf']\n"
     ]
    }
   ],
   "source": [
    "# ─── CELL: Setup all pipeline components ────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from empath import Empath\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# 1. Your existing preprocessing imports (adjust path as needed)\n",
    "from src.preprocessing       import TextPreprocessingPipeline, contains_emoji, flag_urls, flag_mentions\n",
    "from src.feature_engineering import build_features\n",
    "\n",
    "# 2. enrich_df helper\n",
    "def enrich_df(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        texts = X[\"text\"] if \"text\" in X.columns else X.iloc[:,0]\n",
    "    elif hasattr(X, \"ravel\"):\n",
    "        texts = pd.Series(X.ravel())\n",
    "    else:\n",
    "        texts = pd.Series(X)\n",
    "    df_ = pd.DataFrame({\"text\": texts})\n",
    "    df_[\"has_emoji\"] = df_[\"text\"].apply(contains_emoji)\n",
    "    df_ = flag_urls(df_)\n",
    "    df_ = flag_mentions(df_)\n",
    "    df_ = TextPreprocessingPipeline(text_col=\"text\").run(df_)\n",
    "    df_ = build_features(df_)\n",
    "    return df_\n",
    "\n",
    "# 3. EmpathTransformer\n",
    "class EmpathTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cats):\n",
    "        self.categories = cats\n",
    "        self.lexicon    = Empath()\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        rows = []\n",
    "        for txt in pd.Series(X).ravel():\n",
    "            scores = self.lexicon.analyze(txt, normalize=True)\n",
    "            rows.append([scores.get(c,0.0) for c in self.categories])\n",
    "        return np.array(rows)\n",
    "\n",
    "empath_cats = [\"sadness\",\"negative_emotion\",\"loneliness\",\"depression\"]\n",
    "\n",
    "# 4. Sub-pipelines\n",
    "text_pipe = Pipeline([\n",
    "    (\"select_clean\", FunctionTransformer(lambda df: df[\"clean_text\"], validate=False)),\n",
    "    (\"tfidf\",        TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=5))\n",
    "])\n",
    "\n",
    "meta_cols = [\"char_count\",\"word_count\",\"sentiment\",\"has_url\",\"has_emoji\",\"has_reddit_mention\"]\n",
    "meta_pipe = Pipeline([\n",
    "    (\"select_meta\", FunctionTransformer(lambda df: df[meta_cols], validate=False)),\n",
    "    (\"scale_meta\",  StandardScaler())\n",
    "])\n",
    "\n",
    "empath_pipe = Pipeline([\n",
    "    (\"select_clean\", FunctionTransformer(lambda df: df[\"clean_text\"], validate=False)),\n",
    "    (\"empath\",       EmpathTransformer(empath_cats)),\n",
    "    (\"scale_em\",     StandardScaler())\n",
    "])\n",
    "\n",
    "# 5. Full enriched pipeline\n",
    "proto_pipe = Pipeline([\n",
    "    (\"enrich\", FunctionTransformer(enrich_df, validate=False)),\n",
    "    (\"union\",  FeatureUnion([\n",
    "        (\"tfidf\",   text_pipe),\n",
    "        (\"meta\",    meta_pipe),\n",
    "        (\"empath\",  empath_pipe)\n",
    "    ])),\n",
    "    (\"clf\",    LogisticRegression(class_weight=\"balanced\", max_iter=1000))\n",
    "])\n",
    "\n",
    "print(\"✅ proto_pipe defined with steps:\", [name for name,_ in proto_pipe.steps])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c43289ef-b499-4e45-84c9-eca8d9305196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proto_pipe\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(enriched)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 3. Build background feature matrix\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m bg_feats \u001b[38;5;241m=\u001b[39m featurize(bg_df)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 4. Initialize the SHAP LinearExplainer for your LR step\u001b[39;00m\n\u001b[0;32m     18\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mLinearExplainer(\n\u001b[0;32m     19\u001b[0m     proto_pipe\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# your LogisticRegression\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     bg_feats,                       \u001b[38;5;66;03m# background distribution\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     feature_dependence\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindependent\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mfeaturize\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     10\u001b[0m enriched \u001b[38;5;241m=\u001b[39m proto_pipe\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menrich\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(df)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 2b) Run FeatureUnion (TF-IDF, meta, Empath) → returns array (n_samples × n_features)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proto_pipe\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(enriched)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1802\u001b[0m, in \u001b[0;36mFeatureUnion.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1799\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_list:\n\u001b[0;32m   1800\u001b[0m         routed_params[name] \u001b[38;5;241m=\u001b[39m Bunch(transform\u001b[38;5;241m=\u001b[39m{})\n\u001b[1;32m-> 1802\u001b[0m Xs \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[0;32m   1803\u001b[0m     delayed(_transform_one)(trans, X, \u001b[38;5;28;01mNone\u001b[39;00m, weight, params\u001b[38;5;241m=\u001b[39mrouted_params[name])\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, trans, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter()\n\u001b[0;32m   1805\u001b[0m )\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Xs:\n\u001b[0;32m   1807\u001b[0m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1290\u001b[0m, in \u001b[0;36m_transform_one\u001b[1;34m(transformer, X, y, weight, params)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform_one\u001b[39m(transformer, X, y, weight, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call transform and apply weight to output.\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m \n\u001b[0;32m   1271\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;124;03m        This should be of the form ``process_routing()[\"step_name\"]``.\u001b[39;00m\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1290\u001b[0m     res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mtransform(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mtransform)\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# if we have a weight for this transformer, multiply output\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:903\u001b[0m, in \u001b[0;36mPipeline.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    901\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter():\n\u001b[1;32m--> 903\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params[name]\u001b[38;5;241m.\u001b[39mtransform)\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Xt\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2113\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents):\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2099\u001b[0m \n\u001b[0;32m   2100\u001b[0m \u001b[38;5;124;03m    Uses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2111\u001b[0m \u001b[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2112\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2113\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2115\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[0;32m   2116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1661\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Sample a background set from your train data\n",
    "bg_df = X_train.sample(100, random_state=42).to_frame()\n",
    "\n",
    "# 2. Helper to get numeric features for the classifier\n",
    "def featurize(df):\n",
    "    # 2a) Enrich (text→clean_text + meta-flags + counts + sentiment)\n",
    "    enriched = proto_pipe.named_steps[\"enrich\"].transform(df)\n",
    "    # 2b) Run FeatureUnion (TF-IDF, meta, Empath) → returns array (n_samples × n_features)\n",
    "    return proto_pipe.named_steps[\"union\"].transform(enriched)\n",
    "\n",
    "# 3. Build background feature matrix\n",
    "bg_feats = featurize(bg_df)\n",
    "\n",
    "# 4. Initialize the SHAP LinearExplainer for your LR step\n",
    "explainer = shap.LinearExplainer(\n",
    "    proto_pipe.named_steps[\"clf\"],  # your LogisticRegression\n",
    "    bg_feats,                       # background distribution\n",
    "    feature_dependence=\"independent\"\n",
    ")\n",
    "\n",
    "# 5. Pick a test instance to explain\n",
    "inst_df    = X_test.sample(1, random_state=1).to_frame()\n",
    "inst_feats = featurize(inst_df)\n",
    "\n",
    "# 6. Compute SHAP values\n",
    "#    For binary classification, shap_vals has shape (2, n_features).\n",
    "shap_vals = explainer.shap_values(inst_feats)\n",
    "\n",
    "# 7. Get feature names from your FeatureUnion\n",
    "feat_names = proto_pipe.named_steps[\"union\"].get_feature_names_out()\n",
    "\n",
    "# 8. Visualize with a waterfall plot (class 1 = “suicide”)\n",
    "shap.initjs()\n",
    "shap.plots.waterfall(\n",
    "    shap.Explanation(\n",
    "        values        = shap_vals[1][0],         # class-1 shap values for sample 0\n",
    "        base_values   = explainer.expected_value[1],\n",
    "        data          = inst_feats[0],\n",
    "        feature_names = feat_names\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "541cf2ad-c542-4ef5-8294-4bfc718c160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tfidf', 'clf'])\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "pipeline = joblib.load(\"models/logreg_pipeline.pkl\")\n",
    "print(pipeline.named_steps.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f3dc6-1a77-46b3-b645-745d6316271c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
