{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u><strong>SUICIDAL IDEATION DETECTION</strong></u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Problem Statement</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mental health issues, especially depression, are on the rise globally, particularly among young people. Individuals often share their emotional struggles anonymously on online platforms like Reddit before seeking professional help. This project leverages Natural Language Processing (NLP) to analyze Reddit posts and identify potential signs of suicidal ideation, aiming to support early intervention and healthcare responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Stakeholders</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mental health professionals\n",
    "- NGOs focused on mental health\n",
    "- Public health policymakers \n",
    "- Digital mental health platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Ethical Considerations</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Privacy and Consent\n",
    "Reddit posts are publicly accessible, but users may not expect them to be used for mental health analysis.\n",
    "\n",
    "No personally identifiable information (PII), usernames, or post IDs are stored or referenced.\n",
    "\n",
    "Data is anonymized and handled in strict compliance with [Reddit Terms of Service](https://www.redditinc.com/policies/data-api-terms)\n",
    "\n",
    "- ### Psychological Safety\n",
    "The model is designed for academic exploration and public health awareness—not for clinical decision-making.\n",
    "\n",
    "No automatic interventions are triggered based on model predictions.\n",
    "\n",
    "Results are interpreted with caution to avoid misclassifying vulnerable individuals.\n",
    "- ### Bias and Fairness\n",
    "NLP models may underrepresent or misinterpret signals from marginalized communities.\n",
    "\n",
    "Fairness metrics and feature attribution techniques (e.g SHAP) are employed to ensure model decisions aren’t driven by irrelevant demographic indicators.\n",
    "\n",
    "Continuous auditing for linguistic bias is recommended.\n",
    "- ### Transparency and Interpretability\n",
    "The full model pipeline—from preprocessing to predictions—is documented.\n",
    "\n",
    "- ### Human Oversight\n",
    "\n",
    "This tool is not a diagnostic system; human judgment remains central.\n",
    "\n",
    "Suggestions for high-risk content are intended to augment, not replace, mental health professionals.\n",
    "\n",
    "- ### Use and Dissemination\n",
    "\n",
    "The notebook is published with disclaimers clarifying its scope: research and educational use only.\n",
    "\n",
    "Any downstream applications must be evaluated through an ethical review process.\n",
    "\n",
    "Collaboration with mental health experts and community moderators is encouraged for real-world impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Project Objectives</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Use NLP to classify Reddit posts as depression-related or not\n",
    "\n",
    "- Apply interpretable models to understand which words/phrases contribute to depression\n",
    "predictions.\n",
    "\n",
    "- Evaluate model performance using precision, recall, F1-score, and ROC-AUC, with stratified validation to handle potential class imbalance.\n",
    "\n",
    "- Provide recommendations for how health organizations can use the model for early warning system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Data Understanding</u>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Feature extraction\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve\n",
    "\n",
    "# Training\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Utilities\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Standard libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data into dataframe\n",
    "df = pd.read_csv('Suicide_Detection.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of df\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show distribution of the target variable\n",
    "df['class'].value_counts().index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of each class \n",
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Distribution(Suicidal vs non-suicidal)\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.countplot(data=df, x='class', palette=['black', 'blue'])\n",
    "\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset used was sourced from kaggle, which was then loaded into a pandas dataframe. The dataset contains 232,074 entries and 3 columns. The columns include text , the reddit post content, and class which is the target variable that is either 'suicide' or 'non-suicide'. The dataset is perfectly balanced with 116, 037 entries for both 'suicide' and 'non-suicide' classes. This will prevent the model from being biased towards the mojority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA  PREPARATION\n",
    "This phase involves cleaning, preprocessing and transforming the raw text data for modeling\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the unnecessary index column\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates\n",
    "df.drop_duplicates(subset='text', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in each column\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample of 10000 rows for cleaning\n",
    "sample_df = df.sample(100000, random_state=42).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column (cleaned). This allows modifying the text column without loosing raw data\n",
    "sample_df['cleaned'] = sample_df['text']  # assuming 'text' is the right column\n",
    "sample_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which posts contain URLs\n",
    "sample_df['has_url'] = sample_df['text'].str.contains(r'http\\S+|www\\S+|https\\S+', regex=True)\n",
    "\n",
    "# Count how many contain URLs\n",
    "url_count = sample_df['has_url'].sum()\n",
    "url_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect emojis or non-ASCII characters\n",
    "def contains_emoji(text):\n",
    "    return any(ord(char) > 127 for char in text)\n",
    "\n",
    "sample_df['has_emoji'] = sample_df['text'].apply(contains_emoji)\n",
    "\n",
    "# Count how many contain emojis\n",
    "emoji_count = sample_df['has_emoji'].sum()\n",
    "emoji_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for posts that contain r/ or u/(has reddit mentions, usernames, subreddit)\n",
    "sample_df['has_reddit_mention'] = sample_df['text'].str.contains(r'\\bu/\\w+|\\br/\\w+', regex=True)\n",
    "\n",
    "# Count how many\n",
    "mention_count = sample_df['has_reddit_mention'].sum()\n",
    "mention_count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>DATA PRE-PROCESSING</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective 1:**\n",
    "Use NLP to classify Reddit posts as depression-related or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessingPipeline:\n",
    "    def __init__(self, text_col='text'):\n",
    "        self.text_col = text_col\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.token_pattern = r\"[a-zA-Z]+(?:'[a-z]+)?\"\n",
    "        self.tokenizer = RegexpTokenizer(self.token_pattern)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        contractions = {\n",
    "            \"can't\": \"can not\", \"won't\": \"will not\", \"n't\": \" not\",\n",
    "            \"'re\": \" are\", \"'s\": \" is\", \"'m\": \" am\",\n",
    "            \"'ll\": \" will\", \"'ve\": \" have\", \"'d\": \" would\"\n",
    "        }\n",
    "        for contraction, expanded in contractions.items():\n",
    "            text = re.sub(contraction, expanded, text)\n",
    "        return text\n",
    "\n",
    "    def clean_text(self, df):\n",
    "        df = df.copy()\n",
    "        col = self.text_col\n",
    "\n",
    "        df['clean_text'] = (\n",
    "            df[col].str.lower()\n",
    "                   .apply(self.expand_contractions)\n",
    "                   .str.replace(r\"[^\\w\\s']\", \"\", regex=True)  # Keep apostrophes\n",
    "                   .apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
    "        )\n",
    "\n",
    "        # Remove stopwords after cleaning\n",
    "        df['clean_text'] = df['clean_text'].apply(lambda text: ' '.join([\n",
    "            word for word in text.split() if word not in self.stop_words\n",
    "        ]))\n",
    "\n",
    "        # Filter out empty rows\n",
    "        df = df[df['clean_text'].str.strip().str.len() > 0]\n",
    "        return df\n",
    "\n",
    "    def tokenize(self, df):\n",
    "        # Tokenize and lemmatize\n",
    "        df['tokens'] = df['clean_text'].apply(lambda text: [\n",
    "            self.lemmatizer.lemmatize(token)\n",
    "            for token in self.tokenizer.tokenize(text)\n",
    "        ])\n",
    "        return df\n",
    "\n",
    "    def run(self, df):\n",
    "        df = self.clean_text(df)\n",
    "        df = self.tokenize(df)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessingPipeline(text_col='text')\n",
    "df_processed = preprocessor.run(sample_df)\n",
    "# Check before/after stopword removal and lemmatization\n",
    "df_processed[['clean_text', 'tokens']].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finalize dataset for modeling\n",
    "preprocessor = TextPreprocessingPipeline(text_col='text')\n",
    "df = preprocessor.run(df)  # replaces df with the cleaned version\n",
    "# Check before/after stopword removal and lemmatization\n",
    "df[['clean_text', 'tokens']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Exploratory Text Analysis</u>\n",
    "Here we explore patterns in processed text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Most Frequent Words (excluding stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart shows the top 20 most frequent words in your dataset. Examples include:\n",
    "\n",
    "- Emotional verbs: feel, want, need, know, think\n",
    "\n",
    "- Personal pronouns and reflection words: like, people, life, time, day, back\n",
    "\n",
    "- Suggestive terms tied to inner struggle: even, thing, make, get, really"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Emotional Keyword Frequency by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define emotionally charged keywords commonly found in suicidal ideation\n",
    "emotional_keywords = {\n",
    "    'alone', 'tired', 'lost', 'worthless', 'help', 'please', 'die',\n",
    "    'kill', 'empty', 'numb', 'ending', 'gone', 'love', 'need',\n",
    "    'fuck', 'fucking', 'someone'}\n",
    "\n",
    "# Function to count how many emotional keywords appear in a group of posts\n",
    "def count_emotional_keywords(posts):\n",
    "    all_text = ' '.join(posts['clean_text'])  # merge all posts\n",
    "    tokens = all_text.split()\n",
    "    return Counter([word for word in tokens if word in emotional_keywords])\n",
    "\n",
    "# Separate suicidal and non-suicidal posts\n",
    "suicide_posts = df[df['label'] == 1]     # Suicidal\n",
    "nonsuicide_posts = df[df['label'] == 0]  # Non-suicidal\n",
    "\n",
    "# Count emotional keyword occurrences in both groups\n",
    "suicide_counts = count_emotional_keywords(suicide_posts)\n",
    "nonsuicide_counts = count_emotional_keywords(nonsuicide_posts)\n",
    "\n",
    "# Combine into a DataFrame for comparison\n",
    "emotion_df = pd.DataFrame([suicide_counts, nonsuicide_counts], index=['suicide', 'non-suicide']).T.fillna(0)\n",
    "\n",
    "# Plot comparison\n",
    "emotion_df[['suicide', 'non-suicide']].plot(kind='bar', figsize=(12, 6), colormap='coolwarm')\n",
    "plt.title(\"Emotional Keyword Frequency by Class\")\n",
    "plt.xlabel(\"Keyword\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keywords more frequent in suicidal posts**  \n",
    "- “die,” “kill,” “ending” show tall blue bars vs. short red ones—strong signals of intent.  \n",
    "- “alone,” “empty,” “numb” have notably higher blue counts, reflecting isolation and despair.  \n",
    "\n",
    "**Keywords more common in non-suicidal posts**  \n",
    "- “someone,” “love,” “need” might have higher red bars, indicating more general emotional or relational talk.  \n",
    "- Intensifiers (“fuck,” “fucking”) often more frequent in non-suicidal posts when used as expletives rather than self-harm.  \n",
    "\n",
    "**Balanced or neutral keywords**  \n",
    "- “help” or “please” bars are similar in height, suggesting both classes sometimes ask for aid, but context matters (why vs. how).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Feature Engineering</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Sentiment & Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate text to first 500 characters for analysis\n",
    "df['text'] = df['clean_text'].astype(str).str.slice(0, 500)\n",
    "\n",
    "# Add character and word counts\n",
    "df['char_count'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "# Add sentiment scores\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "             \n",
    "df['sentiment'] = df['text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "\n",
    "df[['text', 'char_count', 'word_count', 'sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots comparison of sentiment by class(0 vs 1) check for differences between classes , are suicidal posts more negative\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=df, x='label', y='sentiment', palette=['red', 'blue'])\n",
    "plt.title(\"Sentiment Score by Class\")\n",
    "plt.xlabel(\"Class (0=Non-Suicidal, 1=Suicidal)\")\n",
    "plt.ylabel(\"VADER Compound Sentiment Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suicidal posts carry a consistently more negative tone, with median sentiment around –0.5, compared to medians near 0.0 for non-suicidal content.\n",
    "\n",
    "Suicide cases will have a lower VADER(Valence Aware Dictionary and sentiment) score tending to move towards -1 which shows there is a probability the text is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall distribution of sentiment for all posts combined( are most posts neutral or negative?)\n",
    "plt.hist(df['sentiment'], bins=40, color='steelblue')\n",
    "plt.title(\"Sentiment Score Distribution (VADER)\")\n",
    "plt.xlabel(\"Compound Sentiment Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large peak at negative values (around -1.0) shows many strongly negative posts—likely enriched for suicidal content.\n",
    "\n",
    "A central peak near 0.0 indicates a substantial share of neutral or mixed-sentiment content.\n",
    "\n",
    "A smaller positive peak suggests fewer overtly positive posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare post length between suicidal and non suicidal posts\n",
    "# Boxplot: Character Count by Class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='class', y='char_count', data=df, palette='Blues')\n",
    "\n",
    "plt.title(\"Character Count by Class\", fontsize=14)\n",
    "plt.xlabel(\"Class (0 = Non-Suicidal, 1 = Suicidal)\", fontsize=12)\n",
    "plt.ylabel(\"Character Count\", fontsize=12)\n",
    "\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suicidal posts are consistently longer than non-suicidal ones and show much wider variation in length. Most messages expressing suicidal thoughts exceed the length of typical posts. In contrast, non-suicidal messages remain relatively short and uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Features and labels\n",
    "X = df['clean_text'] \n",
    "y = df['label']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TF-IDF Vectorization (1000 top features, English stopwords removed)\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# Fit on train, transform on train and test\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)\n",
    "\n",
    "# Dimensionality Reduction with TruncatedSVD (LSA/Latent Semantic Analysis)\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_train_vec_reduced = svd.fit_transform(X_train_vec)\n",
    "X_test_vec_reduced = svd.transform(X_test_vec)\n",
    "\n",
    "# Meta Features: char_count, word_count, sentiment\n",
    "meta_cols = ['char_count', 'word_count', 'sentiment']\n",
    "\n",
    "# Align meta features with X_train/X_test using indexes\n",
    "meta_train = df.loc[X_train.index, meta_cols].reset_index(drop=True).to_numpy()\n",
    "meta_test = df.loc[X_test.index, meta_cols].reset_index(drop=True).to_numpy()\n",
    "\n",
    "# Combine text features and meta features\n",
    "X_train_combined = np.hstack([X_train_vec_reduced, meta_train])\n",
    "X_test_combined = np.hstack([X_test_vec_reduced, meta_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>MODELING</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective 2:**\n",
    "Apply interpretable models to understand which words/phrases contribute to depression predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build predictive models using a pipeline approach. This ensures text preprocessing (TF-IDF) and modeling are performed seamlessly. We will use:\n",
    "- Logistic Regression (for interpretability)\n",
    "- Support Vector Machine (for performance)\n",
    "\n",
    "Evaluation metrics include Accuracy, F1-Score, Confusion Matrix, and ROC AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression pipeline\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Define grid\n",
    "logreg_params = {\n",
    "    'tfidf__max_features': [3000, 5000],\n",
    "    'clf__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# GridSearch\n",
    "logreg_grid = GridSearchCV(logreg_pipeline, logreg_params, cv=3, scoring='f1', verbose=1, n_jobs=-1)\n",
    "logreg_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_logreg = logreg_grid.best_estimator_\n",
    "print(\"Best Logistic Regression Parameters:\", logreg_grid.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of logistic regression model\n",
    "y_pred = best_logreg.predict(X_test)\n",
    "y_pred_proba = best_logreg.predict_proba(X_test)[:, 1]\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "ConfusionMatrixDisplay.from_estimator(best_logreg, X_test, y_test, cmap='Blues')\n",
    "plt.title(\"Best Logistic Regression Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For suicidal content (class 1), precision = 0.94 and recall = 0.92, yielding an F1 of 0.93.\n",
    "\n",
    "The confusion matrix shows roughly 32 187 true positives versus 2 622 false negatives, meaning only about 8 % of at-risk posts slip through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier distinguishes suicidal from non-suicidal posts very effectively, with over 90 % accuracy and F1 scores around 0.93. It reliably flags the vast majority of at-risk messages while keeping false alarms at a manageable level. In practice, its strong recall ensures most genuine cries for help are caught, making it a dependable first line of defense in suicide screening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### SVM(Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define SVM pipeline\n",
    "svm_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid\n",
    "svm_params = {\n",
    "    'tfidf__max_features': [3000, 5000],\n",
    "    'clf__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Run GridSearchCV\n",
    "svm_grid = GridSearchCV(svm_pipeline, svm_params, cv=3, scoring='f1', verbose=1, n_jobs=-1)\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_svm = svm_grid.best_estimator_\n",
    "\n",
    "print(\"Best SVM Parameters:\", svm_grid.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of svm\n",
    "y_pred_svm = best_svm.predict(X_test)\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "ConfusionMatrixDisplay.from_estimator(best_svm, X_test, y_test, cmap='Blues')\n",
    "plt.title(\"Best SVM Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SVM reliably distinguishes suicidal from non-suicidal posts with over 90 % accuracy. Its high precision for suicidal posts ensures most flagged items are genuine, while its recall of 0.92 means only about 8 % of at-risk messages are missed—an acceptable balance for initial screening that could be tuned further if minimizing misses is paramount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Neural Network Model \n",
    "\n",
    "We implement a deep learning model using Keras. This model will use embeddings and dense layers to classify suicidal ideation from Reddit posts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['clean_text'])\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_text'])\n",
    "\n",
    "# Pad sequences to uniform length\n",
    "X_nn = pad_sequences(sequences, maxlen=200)\n",
    "y_nn = df['label'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.3, random_state=42, stratify=y_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define parameters\n",
    "vocab_size = 10000\n",
    "embedding_dim = 50\n",
    "max_length = 200\n",
    "\n",
    "# Build the model\n",
    "mlp_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')])  # Binary classification output])\n",
    "\n",
    " #Compile\n",
    "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics='recall')\n",
    "\n",
    "# stops training when validation loss starts increasing\n",
    "early_stop = EarlyStopping(patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train and save history\n",
    "history = mlp_model.fit(X_train_nn, y_train_nn, epochs=5, batch_size=32, callbacks=[early_stop], validation_data=(X_test_nn, y_test_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label=\"Train Acc\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"Val Acc\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label=\"Train Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Val Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # evaluation for neural network\n",
    "# Predict and evaluate\n",
    "y_pred_nn = (mlp_model.predict(X_test_nn) > 0.5).astype(\"int32\")\n",
    "y_pred_nn_proba = mlp_model.predict(X_test_nn).ravel()\n",
    "\n",
    "print(classification_report(y_test_nn, y_pred_nn))\n",
    "\n",
    "# Confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_nn, y_pred_nn, cmap='Blues')\n",
    "plt.title(\"Neural Network Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUNING NEURAL NETWORKS WITH LTSM\n",
    "We use LTSM because\n",
    "1. Sequential understanding\tLSTMs learn word order and context across a sentence (important for meaning).\n",
    "2. Long-range dependency - LSTMs are good at remembering things like: “I don’t want to live” even if the words are far apart.\n",
    "3. Emotion-rich text- Suicidal posts often use phrases where sequence matters more than word presence (e.g., “I’m tired of living” ≠ “Living makes me tired”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a build function for LTSM\n",
    "\n",
    "from keras.metrics import Recall\n",
    "def build_model(optimizer='adam'):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=10000, output_dim=64, input_length=200),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')])\n",
    "#complile\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[Recall()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "#wrap it in kerasclassifier\n",
    "clf = KerasClassifier(\n",
    "    model=build_model,\n",
    "    optimizer='adam', \n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    verbose=0)\n",
    "\n",
    "#set up hyperparameter grid\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'epochs': [3],  \n",
    "    'batch_size': [32, 64]}\n",
    "\n",
    "#perform GridSearchCV\n",
    "grid = GridSearchCV(estimator=clf, param_grid=param_grid, cv=2, scoring='recall', verbose=2, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train_nn, y_train_nn)\n",
    "\n",
    "#Evaluate the best model\n",
    "best_keras = grid_result.best_estimator_\n",
    "print(\"Best Params:\", grid_result.best_params_)\n",
    "\n",
    "# Predictions\n",
    "y_pred_keras = best_keras.predict(X_test_nn) \n",
    "\n",
    "# Report & Confusion Matrix\n",
    "print(\"Keras Classifier Classification Report:\")\n",
    "print(classification_report(y_test_nn, y_pred_keras))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_nn, y_pred_keras, cmap='Oranges')\n",
    "plt.title(\"KerasClassifier Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### EVALUATION OF ALL MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective 3:** \n",
    "Evaluate model performance using precision, recall, F1-score, and ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all model metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# Logistic Regression\n",
    "all_metrics.append({\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision': precision_score(y_test, y_pred),\n",
    "    'Recall': recall_score(y_test, y_pred),\n",
    "    'F1 Score': f1_score(y_test, y_pred),\n",
    "    'AUC': roc_auc_score(y_test, y_pred_proba)})# Use probabilities here\n",
    "\n",
    "# SVM\n",
    "all_metrics.append({\n",
    "    'Model': 'SVM',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_svm),\n",
    "    'Precision': precision_score(y_test, y_pred_svm),\n",
    "    'Recall': recall_score(y_test, y_pred_svm),\n",
    "    'F1 Score': f1_score(y_test, y_pred_svm)})\n",
    "    \n",
    "\n",
    "# MLP Neural Network\n",
    "all_metrics.append({\n",
    "    'Model': 'mlp_model',\n",
    "    'Accuracy': accuracy_score(y_test_nn, y_pred_nn),\n",
    "    'Precision': precision_score(y_test_nn, y_pred_nn),\n",
    "    'Recall': recall_score(y_test_nn, y_pred_nn),\n",
    "    'F1 Score': f1_score(y_test_nn, y_pred_nn),\n",
    "    'AUC': roc_auc_score(y_test_nn, y_pred_nn_proba)}) # Use probabilities here\n",
    "\n",
    "\n",
    "# GridSearch LSTM / KerasClassifier\n",
    "all_metrics.append({\n",
    "    'Model': 'Tuned LSTM (GridSearch)',\n",
    "    'Accuracy': accuracy_score(y_test_nn, y_pred_keras),\n",
    "    'Precision': precision_score(y_test_nn, y_pred_keras),\n",
    "    'Recall': recall_score(y_test_nn, y_pred_keras),\n",
    "    'F1 Score': f1_score(y_test_nn, y_pred_keras),\n",
    "    'AUC': roc_auc_score(y_test_nn, y_pred_keras)})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a comparison table\n",
    "results_df = pd.DataFrame(all_metrics)\n",
    "results_df.set_index('Model', inplace=True)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model comparison\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "results_df.plot(kind='bar', figsize=(12, 7), colormap='Set2')\n",
    "\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective 4:**\n",
    "Provide recommendations for how health organizations can use the model for early warning system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training multiple machine learning models (Logistic Regression, Random Forest, Linear SVC) and a Neural Network (via Keras), the results can be summarized as follows:\n",
    "\n",
    "- **Accuracy**: All models showed competitive performance in classifying the text data.\n",
    "- **Precision & Recall**: Some models performed better with certain classes, showing strengths in detecting specific sentiments.\n",
    "- **ROC Curve**: Models showed good discrimination ability between classes, with AUC close to 1 in many cases.\n",
    "- **Neural Network**: Slightly better on complex patterns, but more sensitive to hyperparameters and data imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  . Rationale for Model Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Logistic Regression**: Chosen for its simplicity and interpretability.\n",
    "- **Random Forest**: Provides robustness and handles feature importance well.\n",
    "- **Linear SVC**: Good for high-dimensional sparse text data.\n",
    "- **Neural Network**: Included to capture non-linear relationships in sequential text.\n",
    "\n",
    "Each model brings different strengths, and comparing them helps determine the most suitable one for production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  . Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our machine learning models demonstrate that text-based analysis is a powerful tool for detecting signs of depression and emotional distress. Among the models tested, Tuned LSTM (GridSearch) showed the highest recall, making it especially useful for applications where catching as many true cases as possible is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  . Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **I. Mental Health Professionals**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation:**\n",
    "Use text-based screening tools (like the models we developed) to **assist early detection** of depressive language in patient communications (e.g., journal entries, chats, intake forms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why:**\n",
    "These tools can flag potential risk cases for follow-up — especially useful in high-volume or remote care settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **II. NGOs Focused on Mental Health**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation:**\n",
    "Deploy these models in **online support forums or chatbots** to identify and respond to individuals showing signs of mental distress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why:**\n",
    "NGOs often operate with limited staff. Automated tools allow them to **prioritize high-risk users** for quicker human intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **III. Public Health Policymakers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation:**\n",
    "Invest in **scalable, AI-powered mental health monitoring tools** to be used in public health campaigns and services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why:**\n",
    "These models can help **track population-level mental health trends** from anonymized data (e.g., social media), guiding better-targeted mental health policies and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IV. Digital Mental Health Platforms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation:**\n",
    "Integrate the most accurate model into **real-time mental health assessment tools** in apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why:**\n",
    "Adds value by improving app reliability and **customizing support recommendations** for users based on subtle linguistic cues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
